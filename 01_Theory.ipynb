{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Assignment - Theoretical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. What is a Decision Tree, and how does it work?  \n",
    "- A Decision Tree is a supervised learning algorithm used for classification and regression.  \n",
    "\n",
    "- It splits the data into branches based on feature values, forming a tree-like structure.  \n",
    "\n",
    "- The tree consists of decision nodes (where splits occur) and leaf nodes (final predictions).  \n",
    "\n",
    "- It follows a recursive splitting process to maximize information gain or minimize impurity.  \n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "### 2. What are impurity measures in Decision Trees?  \n",
    "- Impurity measures indicate the disorder in a dataset at a node.  \n",
    "\n",
    "- Common impurity measures include **Gini Impurity** and **Entropy**.  \n",
    "\n",
    "- Lower impurity means better homogeneity of data in a node.  \n",
    "\n",
    "- These measures guide the tree in making optimal splits.  \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is the mathematical formula for Gini Impurity?  \n",
    "\n",
    "  - The Gini Impurity measures the probability of misclassifying a randomly chosen element:\n",
    "\n",
    "Gini  =\n",
    "\n",
    "![alt text](gini.png)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What is the mathematical formula for Entropy?\n",
    "\n",
    "\n",
    "![alt text](entropy.png)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. What is Information Gain, and how is it used in Decision Trees?  \n",
    "- **Information Gain (IG)** measures the reduction in impurity after a split.  \n",
    "- It is calculated as:  \n",
    "\n",
    "![alt text](dt.png)\n",
    "\n",
    "- The split with the highest information gain is chosen.  \n",
    "\n",
    "- It ensures the tree makes the most informative decisions.  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6. What is the difference between Gini Impurity and Entropy?  \n",
    "\n",
    "- **Gini Impurity** measures the probability of misclassification, while **Entropy** measures information disorder.  \n",
    "\n",
    "- Entropy is more computationally expensive due to logarithmic calculations. \n",
    "\n",
    "- Gini is often preferred in decision trees due to its simplicity.  \n",
    "\n",
    "- Both methods lead to similar results but may differ in splitting behavior.  \n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 7. What is the mathematical explanation behind Decision Trees?  \n",
    "\n",
    "- Decision Trees use recursive binary splitting based on impurity measures. \n",
    "\n",
    "- The **best split** is chosen using Information Gain, Gini Impurity, or Entropy.  \n",
    "\n",
    "- The process continues until a stopping criterion is met (e.g., max depth, min samples per leaf).  \n",
    "\n",
    "- A tree can be pruned to prevent overfitting.  \n",
    "\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 8. What is Pre-Pruning in Decision Trees?  \n",
    "\n",
    "- Pre-Pruning stops tree growth early to avoid overfitting.  \n",
    "\n",
    "- It applies constraints like **maximum depth** or **minimum samples per leaf**.  \n",
    "\n",
    "- It reduces model complexity and improves generalization.  \n",
    "\n",
    "- However, it may stop before finding the best splits.  \n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 9. What is Post-Pruning in Decision Trees?  \n",
    "\n",
    "- Post-Pruning removes branches after the tree is fully grown.  \n",
    "\n",
    "- It prunes nodes that do not improve model performance on validation data.  \n",
    "\n",
    "- Common methods include **cost complexity pruning** and **reduced error pruning**.  \n",
    "\n",
    "- It helps balance bias and variance for better generalization.  \n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 10. What is the difference between Pre-Pruning and Post-Pruning?  \n",
    "\n",
    "- **Pre-Pruning** stops the tree from growing beyond certain limits, while **Post-Pruning** trims an overgrown tree.  \n",
    "\n",
    "- Pre-Pruning is **proactive**, while Post-Pruning is **reactive**.  \n",
    "\n",
    "- Pre-Pruning may miss important splits, while Post-Pruning ensures optimal simplification.  \n",
    "\n",
    "- Post-Pruning typically yields better results by analyzing tree performance.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 11. What is a Decision Tree Regressor?  \n",
    "\n",
    "- A **Decision Tree Regressor** is used for predicting continuous values.  \n",
    "\n",
    "- Instead of classification, it minimizes the variance in target values.  \n",
    "\n",
    "- Splits are chosen based on metrics like **Mean Squared Error (MSE)**.  \n",
    "\n",
    "- It works well on non-linear relationships but may overfit.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 12. What are the advantages and disadvantages of Decision Trees?  \n",
    "\n",
    "**Advantages:**  \n",
    "\n",
    "- Simple and easy to interpret.  \n",
    "\n",
    "- Handles both numerical and categorical data.  \n",
    "\n",
    "- Requires minimal data preprocessing.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Disadvantages:**  \n",
    "\n",
    "- Prone to overfitting without pruning.  \n",
    "\n",
    "- Sensitive to small changes in data.  \n",
    "\n",
    "- Greedy algorithm may not find the optimal tree.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 13. How does a Decision Tree handle missing values?  \n",
    "\n",
    "- It can **ignore missing values** and choose the best split based on available data.  \n",
    "\n",
    "- It can **assign missing values to the most frequent category** (for categorical data).  \n",
    "\n",
    "- Some implementations use **surrogate splits** to handle missing values.  \n",
    "\n",
    "- Missing values can also be imputed before training.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 14.  How does a Decision Tree handle categorical features?  \n",
    "\n",
    "- It uses **one-hot encoding** or **label encoding** to convert categories into numerical values.  \n",
    "\n",
    "- Some implementations can handle categorical splits directly (e.g., CART for categorical features).  \n",
    "\n",
    "- Feature selection is based on Information Gain or Gini Impurity.  \n",
    "\n",
    "- It creates binary or multi-way splits depending on the implementation.  \n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 15. What are some real-world applications of Decision Trees?  \n",
    "\n",
    "- **Medical Diagnosis:** Identifying diseases based on symptoms.  \n",
    "\n",
    "- **Customer Segmentation:** Categorizing customers for marketing.  \n",
    "\n",
    "- **Fraud Detection:** Detecting fraudulent transactions.  \n",
    "\n",
    "- **Credit Scoring:** Assessing loan eligibility based on customer profiles.  \n",
    "\n",
    "- **Recommendation Systems:** Suggesting products based on user preferences.  \n",
    "\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
